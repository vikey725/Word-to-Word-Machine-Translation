{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "allied-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pickle\n",
    "import scipy\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ultimate-medication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the English to French training dictionary is 5000\n",
      "The length of the English to French test dictionary is 5000\n"
     ]
    }
   ],
   "source": [
    "def get_dict(file_name):\n",
    "    \"\"\"\n",
    "    This function returns the english to french dictionary given a file where the each column corresponds to a word.\n",
    "    Check out the files this function takes in your workspace.\n",
    "    \"\"\"\n",
    "    my_file = pd.read_csv(file_name, delimiter=' ')\n",
    "    etof = {}  # the english to french dictionary to be returned\n",
    "    for i in range(len(my_file)):\n",
    "        # indexing into the rows.\n",
    "        en = my_file.loc[i][0]\n",
    "        fr = my_file.loc[i][1]\n",
    "        etof[en] = fr\n",
    "\n",
    "    return etof\n",
    "\n",
    "en_embeddings_subset = pickle.load(open(\"en_embeddings.p\", \"rb\"))\n",
    "fr_embeddings_subset = pickle.load(open(\"fr_embeddings.p\", \"rb\"))\n",
    "\n",
    "# loading the english to french dictionaries\n",
    "en_fr_train = get_dict('en-fr.train.txt')\n",
    "print('The length of the English to French training dictionary is', len(en_fr_train))\n",
    "en_fr_test = get_dict('en-fr.test.txt')\n",
    "print('The length of the English to French test dictionary is', len(en_fr_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "experimental-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(en_fr_train, en_embeddings_subset, fr_embeddings_subset):\n",
    "    X = []\n",
    "    Y = []\n",
    "    en_words = []\n",
    "    fr_words = []\n",
    "    for en_word, fr_word in en_fr_train.items():\n",
    "        if en_word in en_embeddings_subset and fr_word in fr_embeddings_subset:\n",
    "            X.append(en_embeddings_subset[en_word])\n",
    "            en_words.append(en_word)\n",
    "            Y.append(fr_embeddings_subset[fr_word])\n",
    "            fr_words.append(fr_word)\n",
    "            \n",
    "    return np.vstack(X), np.vstack(Y), en_words, fr_words\n",
    "\n",
    "train_x, train_y, en_train, fr_train = prepare_data(en_fr_train, en_embeddings_subset, fr_embeddings_subset)\n",
    "test_x, test_y, en_test, fr_test = prepare_data(en_fr_test, en_embeddings_subset, fr_embeddings_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "satisfactory-connectivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4932, 300), (4932, 300), (1438, 300), (1438, 300))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape, train_y.shape, test_x.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ordinary-factor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 300)\n",
      "The cost after iter: 24 is 73.26030145149589\n",
      "The cost after iter: 49 is 16.789929834251637\n",
      "The cost after iter: 74 is 5.571954207315096\n",
      "The cost after iter: 99 is 2.4418295980738693\n",
      "The cost after iter: 124 is 1.3639677279886382\n",
      "The cost after iter: 149 is 0.9362463934753547\n",
      "The cost after iter: 174 is 0.749032207248805\n",
      "The cost after iter: 199 is 0.6610885484583374\n",
      "The cost after iter: 224 is 0.6174871317802428\n",
      "The cost after iter: 249 is 0.5949090135050761\n",
      "The cost after iter: 274 is 0.582783302467253\n",
      "The cost after iter: 299 is 0.5760648079309624\n",
      "The cost after iter: 324 is 0.5722409902895709\n",
      "The cost after iter: 349 is 0.570013943662149\n",
      "The cost after iter: 374 is 0.568691179930333\n",
      "The cost after iter: 399 is 0.5678924068630982\n",
      "The cost after iter: 424 is 0.5674033310590154\n",
      "The cost after iter: 449 is 0.5671004146861212\n",
      "The cost after iter: 474 is 0.5669110058455283\n",
      "The cost after iter: 499 is 0.5667916372153659\n"
     ]
    }
   ],
   "source": [
    "NUM_ITER = 500\n",
    "BATCH_SIZE = 32\n",
    "def train(train_x, train_y):\n",
    "    R = np.random.rand(EMBEDDING_DIM, EMBEDDING_DIM)\n",
    "    print(R.shape)\n",
    "    num_batches = np.ceil(train_x.shape[0]/BATCH_SIZE)\n",
    "    \n",
    "    for iter_idx in range(NUM_ITER):\n",
    "        start_idx = 0\n",
    "        end_idx = start_idx + BATCH_SIZE\n",
    "        loss = 0\n",
    "        for batch_idx in range(int(num_batches)):\n",
    "            x_batch = train_x[start_idx:end_idx, :]\n",
    "            y_batch = train_y[start_idx:end_idx, :]\n",
    "            \n",
    "            loss += np.sum((np.dot(x_batch, R)-y_batch)**2)/train_x.shape[0]\n",
    "            start_idx += BATCH_SIZE\n",
    "            end_idx += BATCH_SIZE\n",
    "            \n",
    "            R -= np.dot(x_batch.transpose(),np.dot(x_batch,R)-y_batch)*(2/train_x.shape[0])\n",
    "        \n",
    "        if (iter_idx + 1)%25 == 0:\n",
    "            print(f\"The cost after iter: {iter_idx} is {loss}\")\n",
    "    \n",
    "    return R\n",
    "\n",
    "R = train(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-winning",
   "metadata": {},
   "source": [
    "1. Each plane divides the space to $2$ parts.\n",
    "2. So $n$ planes divide the space into $2^{n}$ hash buckets.\n",
    "3. We want to organize 10,000 document vectors into buckets so that every bucket has about $~16$ vectors.\n",
    "4. For that we need $\\frac{10000}{16}=625$ buckets.\n",
    "5. We're interested in $n$, number of planes, so that $2^{n}= 625$. Now, we can calculate $n=\\log_{2}625 = 9.29 \\approx 10$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adolescent-michigan",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) *np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "level-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_nearest_neighbours(v, candidates, k=1):\n",
    "    \n",
    "    neighbours = []\n",
    "    for candidate in candidates:\n",
    "        sim = cosine_similarity(v, candidate)\n",
    "        neighbours.append(sim)\n",
    "        \n",
    "    sorted_idxs = np.argsort(neighbours)\n",
    "    \n",
    "    return sorted_idxs[-k:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "legitimate-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,) (300, 300)\n",
      "worn\n",
      "['laiton', 'épaule', 'portait', 'capuchon', 'sculptés', 'porté', 'rembourrage', 'nettoyés', 'vêtement', 'habillés']\n",
      "porté\n"
     ]
    }
   ],
   "source": [
    "# predict for a word\n",
    "def predict(word, word_vec, R, candidates, fr_word_list):\n",
    "    print(word_vec.shape, R.shape)\n",
    "    word_idx = k_nearest_neighbours(np.dot(word_vec, R), candidates, k=10)\n",
    "\n",
    "    print(word)\n",
    "    print([fr_test[int(idx)] for idx in word_idx])\n",
    "    \n",
    "predict(en_test[30], test_x[30], R, test_y, fr_test)\n",
    "print(fr_test[30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "coastal-tyler",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.11961057023644"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_accuracy(test_x, test_y):\n",
    "    preds = np.dot(test_x, R)\n",
    "    correct = 0\n",
    "    for i in range(len(test_x)):\n",
    "        pred_idx = k_nearest_neighbours(preds[i], test_y)\n",
    "        \n",
    "        if int(pred_idx) == i:\n",
    "            correct += 1\n",
    "            \n",
    "    return correct*100/len(test_x)\n",
    "            \n",
    "    \n",
    "test_accuracy(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-accommodation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow-gpu)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
